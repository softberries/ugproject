Data from sql lite database found here: https://www.kaggle.com/c/reddit-comments-may-2015/scripts

select * from may2015 limit 1

All Columns:

created_utc,
ups,
subreddit_id,
link_id,
name,
score_hidden,
author_flair_css_class,
author_flair_text,
subreddit,
id,
removal_reason,
gilded,
downs,
archived,
author,
score,
retrieved_on,
body,
distinguished,
edited,
controversiality,
parent_id

Example JSON entry as in the original datasource (https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment):

{
   "gilded":0,
   "author_flair_text":"Male",
   "author_flair_css_class":"male",
   "retrieved_on":1425124228,
   "ups":3,
   "subreddit_id":"t5_2s30g",
   "edited":false,
   "controversiality":0,
   "parent_id":"t1_cnapn0k",
   "subreddit":"AskMen",
   "body":"I can't agree with passing the blame, but I'm glad to hear it's at least helping you with the anxiety. I went the other direction and started taking responsibility for everything. I had to realize that people make mistakes including myself and it's gonna be alright. I don't have to be shackled to my mistakes and I don't have to be afraid of making them. ",
   "created_utc":"1420070668",
   "downs":0,
   "score":3,
   "author":"TheDukeofEtown",
   "archived":false,
   "distinguished":null,
   "id":"cnasd6x",
   "score_hidden":false,
   "name":"t1_cnasd6x",
   "link_id":"t3_2qyhmp"
}

There are 54504410 records in the database (sql lite) and it weights 7.9GB (7-zipped)

54504410 - 7900 MB
x - 100 MB

x = (54504410 * 100) / 7900 = 689929 -> ~700000 records
To get 100MB we would need to select about 700000 records from the sql lite database and convert them to JSON array
